# Beyond Keywords: Modern Search Algorithms Explained

*Written by Alex â€¢ JulyÂ 26,Â 2025*

Searching is one of the most fundamental problems in computer science. Whether you are looking for a value in an array, the fastest route on a map, or the most relevant document in a knowledge base, you are running a **search algorithm**.

This post walks through the evolution of searchâ€”from the simplest linear scan to modern hybrid approaches that combine dense vector similarity with lexical ranking. Feel free to skip around or dig into the code snippets!

---

## 1Â Â· Classic Dataâ€‘Structure Search

### 1.1Â Linear Search

*Best when the data are small or unsorted.*

```python
for i, item in enumerate(arr):
    if item == target:
        return i
return -1
```

* **TimeÂ Complexity:**Â O(*n*)
* **SpaceÂ Complexity:**Â O(1)

### 1.2Â Binary Search

*Requires the data to be **sorted**.*

```python
from typing import List

def binary_search(arr: List[int], target: int) -> int:
    lo, hi = 0, len(arr) - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if arr[mid] == target:
            return mid
        if arr[mid] < target:
            lo = mid + 1
        else:
            hi = mid - 1
    return -1
```

* **TimeÂ Complexity:**Â O(logÂ *n*)
* **SpaceÂ Complexity:**Â O(1)

### 1.3Â Hashâ€‘Table Lookup

| Operation       | Average Time | WorstÂ Case |
| --------------- | ------------ | ---------- |
| insert / search | O(1)         | O(*n*)     |

Collisions are handled via chaining or open addressing.

### 1.4Â Treeâ€‘Based Search (BST, AVL, Redâ€‘Black)

Selfâ€‘balancing trees guarantee O(logÂ *n*) search even after many inserts/deletes.

---

## 2Â Â· Graph Search

When the "data" are nodes connected by edges, you need a graph algorithm.

| Algorithm                  | Useâ€‘Case                              | Optimal?          | Cost              |
| -------------------------- | ------------------------------------- | ----------------- | ----------------- |
| Breadthâ€‘FirstÂ Search (BFS) | Unweighted shortest path              | âœ”                 | O(*V*Â +Â *E*)      |
| Depthâ€‘FirstÂ Search (DFS)   | Cycle detection, topological sort     | â€”                 | O(*V*Â +Â *E*)      |
| Dijkstra                   | Weighted shortest path (nonâ€‘negative) | âœ”                 | O(*E*Â logÂ *V*)    |
| A\*                        | Heuristic pathfinding                 | âœ” (if admissible) | O(*E*)Â on average |

```python
import heapq

def dijkstra(graph, src):
    dist = {v: float('inf') for v in graph}
    dist[src] = 0
    pq = [(0, src)]
    while pq:
        d, u = heapq.heappop(pq)
        if d > dist[u]:
            continue
        for v, w in graph[u]:
            nd = d + w
            if nd < dist[v]:
                dist[v] = nd
                heapq.heappush(pq, (nd, v))
    return dist
```

---

## 3Â Â· Informationâ€‘Retrieval Search

### 3.1Â TFâ€‘IDF

The classic vectorâ€‘space model that weighs terms by frequency and rarity.

### 3.2Â BM25

A probabilistic ranking function that improves on TFâ€‘IDF by normalizing for document length. Widely used in search engines and available in SQLite FTS5.

```sql
SELECT docid, bm25(fts) AS score
FROM   fts
WHERE  fts MATCH 'neural network'
ORDER  BY score
LIMIT  10;
```

---

## 4Â Â· Semantic Search with Embeddings

Vector embeddings map text into a highâ€‘dimensional space where cosine similarity approximates semantic closeness.

1. **Embed** passages with a model like `sentence-transformers/all-MiniLM-L6-v2`.
2. **Store** them in a vector index (e.g., FAISS, `sqlite-vec`).
3. **Query** by embedding the user input and retrieving nearest neighbors.

Pros:

* Handles synonyms and paraphrases.

Cons:

* May ignore exact keywords ("H2O" vs "water").

---

## 5Â Â· Hybrid Search & Reciprocal RankÂ Fusion (RRF)

Combines lexical (BM25) and semantic (vector) scores.

```python
def rrf_score(ranks, k=60):
    return sum(1/(k + r) for r in ranks)
```

Hybrid search often yields the best of both worldsâ€”precise keyword matching and semantic generalization.

---

## 6Â Â· Evaluating Search Quality

* **Precision @Â *k*** â€” How many of the top *k* results are relevant?
* **Recall @Â *k*** â€” How many relevant results did we retrieve out of all
  that exist?
* **MRR** (Mean Reciprocal Rank) â€” Useful when there is exactly one correct
  answer per query.
* **nDCG** (Normalized Discounted Cumulative Gain) â€” Accounts for graded
  relevance.

---

## 7Â Â· Practical Tips

1. **Chunking matters**: overlap chunks to avoid splitting sentences.
2. **Stopâ€‘word handling**: keep vs remove depends on the model.
3. **Caching embeddings**: speeds up reâ€‘indexing.
4. **Monitor drift**: refresh embeddings when the domain language shifts.
5. **Guardrails**: apply filters (date, language, user permissions) before ranking.

---

## 8Â Â· Further Reading

* Manning, C.Â D., Raghavan, P., &Â SchÃ¼tze,Â H. *Introduction to Information Retrieval*.
* Jurafsky, D., &Â Martin, J.Â H. *Speech and Language Processing*.
* **SQLiteÂ FTS5** documentation [https://sqlite.org/fts5.html](https://sqlite.org/fts5.html)
* **FAISS**: Facebook AI Similarity Search [https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss)
* Cooperman,Â A. *The RRF Keynote* (SIGIRÂ 2023).

---

Thanks for reading! Feel free to share questions, corrections, or war stories about search in the comments below. ðŸš€
